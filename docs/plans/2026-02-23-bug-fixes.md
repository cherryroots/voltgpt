# Bug Fixes Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Fix 11 confirmed bugs spanning crashes, data races, logic errors, and resource leaks across the voltgpt codebase.

**Architecture:** Each fix is a minimal, targeted change. Tests are added before fixing. Every task ends with a commit. No refactoring beyond what is required to fix the bug.

**Tech Stack:** Go 1.23+, `database/sql`, `sqlite-vec`, `sync` package, `time` package, `discordgo`.

---

## Quick Reference

Run all tests: `/usr/local/go/bin/go test ./... -timeout 60s`
Run tests for one package: `/usr/local/go/bin/go test voltgpt/internal/<pkg> -timeout 60s -v`
Race detector: `/usr/local/go/bin/go test -race voltgpt/internal/<pkg> -timeout 60s`
Build check: `/usr/local/go/bin/go build -o voltgpt`

---

## Task 1: Fix division-by-zero and chunk-index panic in VideoToBase64Images

**Context:** Two adjacent bugs in the same function in `internal/utility/video.go`.

**Bug A (line 58):** When `totalFrames == 1`, the timestamp loop computes `float64(0) / float64(0)` = NaN. NaN is passed to ffmpeg as a filter expression, producing no frames. The function then returns an error. `totalFrames` is clamped to a minimum of 1 at line 51, so this happens for any video shorter than ~0.34 seconds.

**Bug B (lines 131–134):** When iterating extracted frames, if `frames[0]` is nil (extraction failed for index 0) and the first successful frame is at index `k` where `k % 81 != 0`, no chunk has been appended yet. `b[len(b)-1]` panics with index out of range. This is made more likely by Bug A (short videos where frame 0 returns no output from ffmpeg).

**Files:**
- Modify: `internal/utility/video.go`
- Test: `internal/utility/video_test.go`

**Step 1: Write the failing tests**

Add two tests to `internal/utility/video_test.go`. The test file already imports `os` and `testing`; add `bytes` to imports.

```go
func TestVideoToBase64ImagesFrameTimestamps_SingleFrame(t *testing.T) {
	// When totalFrames is clamped to 1, timestamp must not be NaN.
	// usableDuration * 0.98 * 3fps < 1 → duration < 0.34s triggers this.
	// We test the timestamp logic directly via a table-driven approach.
	// The timestamp for frame 0 of 1 must be > 0 and finite.
	duration := 0.1
	usableDuration := duration * 0.98
	totalFrames := int(usableDuration * 3.0) // == 0, clamped to 1
	if totalFrames < 1 {
		totalFrames = 1
	}
	var timestamps []float64
	if totalFrames == 1 {
		timestamps = []float64{usableDuration / 2}
	} else {
		for i := range totalFrames {
			timestamps = append(timestamps, (float64(i)/float64(totalFrames-1))*usableDuration)
		}
	}
	if len(timestamps) != 1 {
		t.Fatalf("expected 1 timestamp, got %d", len(timestamps))
	}
	ts := timestamps[0]
	if ts != ts { // NaN check: NaN != NaN
		t.Errorf("timestamp is NaN")
	}
	if ts <= 0 {
		t.Errorf("timestamp %f must be > 0", ts)
	}
}

func TestVideoToBase64ImagesChunkIndex_NilFirstFrame(t *testing.T) {
	// Simulate the chunk grouping with frames[0] == nil, frames[1] != nil.
	// Without the fix, b[len(b)-1] panics. With the fix it must not panic.
	frames := make([]*bytes.Buffer, 2)
	frames[0] = nil
	frames[1] = bytes.NewBufferString("data")

	b := [][81]*bytes.Buffer{}
	for i, frame := range frames {
		if frame == nil {
			continue
		}
		if len(b) == 0 || i%81 == 0 {
			b = append(b, [81]*bytes.Buffer{})
		}
		chunkIndex := len(b) - 1
		b[chunkIndex][i%81] = frame
	}
	if len(b) == 0 {
		t.Fatal("expected at least one chunk")
	}
	if b[0][1] == nil {
		t.Error("expected frame at position 1 in chunk 0")
	}
}
```

**Step 2: Run to verify they fail (or confirm the logic compiles correctly)**

```
/usr/local/go/bin/go test voltgpt/internal/utility -run "TestVideoToBase64Images" -v -timeout 30s
```

The chunk test will pass with the new logic written directly in the test. What we need to confirm is that the _production_ code does NOT yet have this logic. The timestamp test exercises the corrected logic inline; the actual bug is in `video.go`.

**Step 3: Apply fix A — single-frame timestamp**

In `internal/utility/video.go`, replace lines 56–60:

```go
// Before:
var timestamps []float64
for i := range totalFrames {
    timestamp := (float64(i) / float64(totalFrames-1)) * usableDuration
    timestamps = append(timestamps, timestamp)
}
```

```go
// After:
var timestamps []float64
if totalFrames == 1 {
    timestamps = []float64{usableDuration / 2}
} else {
    for i := range totalFrames {
        timestamp := (float64(i) / float64(totalFrames-1)) * usableDuration
        timestamps = append(timestamps, timestamp)
    }
}
```

**Step 4: Apply fix B — chunk index guard**

In `internal/utility/video.go`, replace lines 131–132:

```go
// Before:
if i%81 == 0 {
    b = append(b, [81]*bytes.Buffer{})
}
```

```go
// After:
if len(b) == 0 || i%81 == 0 {
    b = append(b, [81]*bytes.Buffer{})
}
```

**Step 5: Build and run tests**

```
/usr/local/go/bin/go build -o voltgpt && /usr/local/go/bin/go test voltgpt/internal/utility -run "TestVideoToBase64Images" -v -timeout 30s
```

Expected: both tests pass, build succeeds.

**Step 6: Commit**

```bash
git add internal/utility/video.go internal/utility/video_test.go
git commit -m "fix(utility): prevent NaN timestamp and chunk-index panic in VideoToBase64Images"
```

---

## Task 2: Fix panic in SplitParagraph when code block has no trailing newline

**Context:** `internal/utility/messages.go` line 35. When a message is split and the first half ends with an unclosed code fence (odd number of ` ``` `), the code tries to extract the language tag. If there is no newline after the fence, `strings.Index` returns -1, and `lastCodeBlock[:-1]` panics.

**Files:**
- Modify: `internal/utility/messages.go`
- Test: `internal/utility/messages_test.go`

**Step 1: Write the failing test**

Add to the existing `TestSplitParagraph` table in `internal/utility/messages_test.go`:

```go
{
    // Code block with no newline after the opening fence — must not panic.
    name:          "code block no trailing newline",
    message:       strings.Repeat("a", 1990) + "```",
    wantFirstPart: strings.Repeat("a", 1990) + "```" + "```",
    wantLastPart:  "```\n",
},
```

**Step 2: Run to verify it panics**

```
/usr/local/go/bin/go test voltgpt/internal/utility -run "TestSplitParagraph/code_block_no_trailing_newline" -v -timeout 10s
```

Expected: panic / test failure.

**Step 3: Apply the fix**

In `internal/utility/messages.go`, replace line 35:

```go
// Before:
languageCode := lastCodeBlock[:strings.Index(lastCodeBlock, "\n")]
```

```go
// After:
newlineIdx := strings.Index(lastCodeBlock, "\n")
if newlineIdx < 0 {
    newlineIdx = len(lastCodeBlock)
}
languageCode := lastCodeBlock[:newlineIdx]
```

**Step 4: Run tests**

```
/usr/local/go/bin/go test voltgpt/internal/utility -run "TestSplitParagraph" -v -timeout 10s
```

Expected: all SplitParagraph tests pass.

**Step 5: Commit**

```bash
git add internal/utility/messages.go internal/utility/messages_test.go
git commit -m "fix(utility): prevent panic in SplitParagraph when code fence has no trailing newline"
```

---

## Task 3: Fix HTTP response body leak in hasher.getFile

**Context:** `internal/hasher/hasher.go` lines 329–332. When the HTTP response status is not 200 OK, the function returns early before `defer resp.Body.Close()` is reached, leaking the underlying TCP connection. Under sustained error conditions this exhausts the HTTP connection pool.

**Files:**
- Modify: `internal/hasher/hasher.go`
- Test: `internal/hasher/hasher_test.go`

**Step 1: Write the failing test**

Add to `internal/hasher/hasher_test.go`. Look at the existing test setup — `database = nil` is fine here since `getFile` does not touch the DB.

```go
func TestGetFile_NonOKStatusClosesBody(t *testing.T) {
	// Serve a 404 response. The body must still be drained/closed,
	// meaning a second request on the same server succeeds (connection reused).
	srv := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusNotFound)
		w.Write([]byte("not found"))
	}))
	defer srv.Close()

	_, err := getFile(srv.URL)
	if err == nil {
		t.Fatal("expected error for 404 response, got nil")
	}
	if !strings.Contains(err.Error(), "bad status") {
		t.Errorf("unexpected error: %v", err)
	}
}
```

Imports needed: `"net/http"`, `"net/http/httptest"`, `"strings"`. Check existing imports and add missing ones.

**Step 2: Run to verify the test passes (it won't catch the leak directly, but confirms error path)**

```
/usr/local/go/bin/go test voltgpt/internal/hasher -run "TestGetFile_NonOKStatusClosesBody" -v -timeout 10s
```

**Step 3: Apply the fix**

In `internal/hasher/hasher.go`, move `defer resp.Body.Close()` to immediately after the error check, before the status check:

```go
// Before:
resp, err := client.Get(url)
if err != nil {
    return buf, err
}
if resp.StatusCode != http.StatusOK {
    return buf, fmt.Errorf("bad status: %s", resp.Status)
}
defer resp.Body.Close()
```

```go
// After:
resp, err := client.Get(url)
if err != nil {
    return buf, err
}
defer resp.Body.Close()
if resp.StatusCode != http.StatusOK {
    return buf, fmt.Errorf("bad status: %s", resp.Status)
}
```

**Step 4: Run all hasher tests**

```
/usr/local/go/bin/go test voltgpt/internal/hasher -timeout 30s -v
```

Expected: all pass.

**Step 5: Commit**

```bash
git add internal/hasher/hasher.go internal/hasher/hasher_test.go
git commit -m "fix(hasher): close HTTP response body on non-OK status to prevent connection leak"
```

---

## Task 4: Fix data races in `draw` and `hash_server` slash commands

**Context:** Two data races in `internal/handler/commands.go`.

**Race A (lines 122, 130):** The outer `resolution` variable is written inside each goroutine in the `draw` command's goroutine fan. All goroutines compute the same value (from `imgs[0]`), but concurrent writes are a data race. Fix: compute `resolution` once before spawning goroutines.

**Race B (lines 398, 403):** The `msgCount` and `hashCount` variables are written inside concurrent goroutines in `hash_server` without any locking. Fix: accumulate locally in each goroutine, then merge under a mutex.

Note: there are no unit tests for the `handler` package due to Discord session mocking complexity. Verify with the race detector by extracting the patterns, or rely on `-race` in the full binary test. The fixes themselves are straightforward.

**Files:**
- Modify: `internal/handler/commands.go`

**Step 1: Fix Race A — compute `resolution` before the goroutine loop**

In `internal/handler/commands.go`, find the `draw` command handler. The `resolution` variable is written at lines 122 and 130 inside the goroutine (inside the `if imgFilled` block). Extract that computation to before the `wg.Add`/goroutine loop.

Before the `for range amount` goroutine loop, add:

```go
if imgFilled && len(imgs) > 0 {
    aspectRatio, err := utility.GetAspectRatio(*imgs[0])
    if err != nil {
        _, err = discord.SendFollowup(s, i, err.Error())
        if err != nil {
            log.Println(err)
        }
        return
    }
    if aspectRatio >= 1 {
        H := 2048
        W := int(float64(H) * aspectRatio)
        if W > 4096 {
            W = 4096
            H = max(int(float64(W)/aspectRatio), 1024)
        }
        resolution = fmt.Sprintf("%d*%d", W, H)
    } else {
        W := 2048
        H := int(float64(W) / aspectRatio)
        if H > 4096 {
            H = 4096
            W = max(int(float64(H)*aspectRatio), 1024)
        }
        resolution = fmt.Sprintf("%d*%d", W, H)
    }
}
```

Then inside the goroutine's `if imgFilled` block, delete the aspect ratio computation and the two `resolution = fmt.Sprintf(...)` lines (the goroutine already has `resolution` set). Keep the URL validation loop, base64 conversion, and API call; just remove the aspect ratio fetch and resolution assignment.

**Step 2: Fix Race B — local accumulation with mutex in `hash_server`**

In `internal/handler/commands.go`, find the `hash_server` handler. Replace the goroutine body:

```go
// Before:
var wg sync.WaitGroup
for messages := range messageChannel {
    wg.Add(1)
    go func(messages []*discordgo.Message) {
        defer wg.Done()
        msgCount += len(messages)
        for _, message := range messages {
            if utility.HasImageURL(message) || utility.HasVideoURL(message) {
                options := hasher.HashOptions{Store: true}
                _, count := hasher.HashAttachments(message, options)
                hashCount += count
            }
        }
        _, err := discord.EditMessage(s, hashedStatus, fmt.Sprintf(..., msgCount, hashCount))
        ...
    }(messages)
}
```

```go
// After:
var wg sync.WaitGroup
var countMu sync.Mutex
for messages := range messageChannel {
    wg.Add(1)
    go func(messages []*discordgo.Message) {
        defer wg.Done()
        localMsg := len(messages)
        localHash := 0
        for _, message := range messages {
            if utility.HasImageURL(message) || utility.HasVideoURL(message) {
                options := hasher.HashOptions{Store: true}
                _, count := hasher.HashAttachments(message, options)
                localHash += count
            }
        }
        countMu.Lock()
        msgCount += localMsg
        hashCount += localHash
        currentMsg := msgCount
        currentHash := hashCount
        countMu.Unlock()
        _, err := discord.EditMessage(s, hashedStatus, fmt.Sprintf("Status: ongoing\nThreads included: %t\nHashing until: %s\nMessages processed: %d\nHashes: %d", threads, endDate.Format("2006/01/02"), currentMsg, currentHash))
        if err != nil {
            log.Println(err)
        }
    }(messages)
}
```

**Step 3: Build and verify with race detector**

```
/usr/local/go/bin/go build -race -o voltgpt
```

Expected: builds cleanly with no race warnings.

**Step 4: Commit**

```bash
git add internal/handler/commands.go
git commit -m "fix(handler): eliminate data races on resolution, msgCount, and hashCount in goroutine fans"
```

---

## Task 5: Fix time.Sleep race in memory.flushBuffer retry logic

**Context:** `internal/memory/extract.go` lines 94–101. `flushBuffer` is a `time.AfterFunc` callback. After releasing `buffersMu` at line 77, it sleeps for 5 seconds on Gemini API failure. During that sleep, a new `Extract` call for the same user creates a fresh buffer entry and starts a new timer. The retry then runs on stale data that has already been replaced, risking double-extraction or data loss.

**Fix:** Remove the inline sleep. On first failure, log and return. The 30-second sliding window means the same content will likely appear in the next buffer anyway. Alternatively, schedule the retry with `time.AfterFunc` so it doesn't block the current timer goroutine and re-locks `buffersMu` to check if the buffer was superseded. The simpler option (drop the retry) is preferred per YAGNI.

**Files:**
- Modify: `internal/memory/extract.go`
- Test: `internal/memory/memory_test.go`

**Step 1: Write a test confirming retry removal compiles and extract path works**

The existing memory tests already cover `flushBuffer` indirectly. Just verify the build succeeds after the change.

**Step 2: Apply the fix**

In `internal/memory/extract.go`, replace lines 92–100:

```go
// Before:
facts, err := extractFacts(ctx, name, combined)
if err != nil {
    log.Printf("memory: extraction attempt 1 failed for %s: %v — retrying in 5s", username, err)
    time.Sleep(5 * time.Second)
    facts, err = extractFacts(ctx, name, combined)
    if err != nil {
        log.Printf("memory: extraction failed after retry for %s, messages lost: %v", username, err)
        return
    }
}
```

```go
// After:
facts, err := extractFacts(ctx, name, combined)
if err != nil {
    log.Printf("memory: extraction failed for %s, messages lost: %v", username, err)
    return
}
```

Also remove the `"time"` import from `extract.go` if `time.Sleep` was its only usage. Check: if `time` is used elsewhere in the file, leave the import.

**Step 3: Build and run memory tests**

```
/usr/local/go/bin/go build -o voltgpt && /usr/local/go/bin/go test voltgpt/internal/memory -timeout 60s -v
```

Expected: build succeeds, all tests pass (Gemini-backed tests skip if `MEMORY_GEMINI_TOKEN` is unset).

**Step 4: Commit**

```bash
git add internal/memory/extract.go
git commit -m "fix(memory): remove blocking time.Sleep from AfterFunc callback to prevent buffer race"
```

---

## Task 6: Fix sqlite-vec ANN post-filter bug in memory retrieval and consolidation

**Context:** `internal/memory/consolidate.go` lines 115–124 and `internal/memory/retrieve.go` lines 45–55. Both use sqlite-vec's `MATCH`+`k=` ANN syntax, which returns the global top-`k` vectors first and only then applies the `JOIN` filter on `user_id`. When other users' facts dominate the top-`k` neighbourhood, per-user queries silently return 0 results. CLAUDE.md confirms: "ANN queries require the `MATCH` + `k=` syntax" but "vec0 tables support full-table scans."

**Fix:** Switch user-scoped queries (`findSimilarFacts` and `Retrieve`) to full-table scans using `vec_distance_cosine()`. `RetrieveGeneral` is already a global search so it stays as-is, but increase its `k` buffer slightly.

**Files:**
- Modify: `internal/memory/consolidate.go`
- Modify: `internal/memory/retrieve.go`
- Test: `internal/memory/memory_test.go`

**Step 1: Write a test that exercises per-user filtering**

Add to `internal/memory/memory_test.go` (white-box, `package memory`):

```go
func TestFindSimilarFacts_UserScoped(t *testing.T) {
	// If facts from other users are nearer in vector space, findSimilarFacts
	// must still return only facts belonging to the target user.
	db, err := db.Open(":memory:")
	if err != nil {
		t.Fatal(err)
	}
	database = db.DB

	// Insert a fact for user 1 and a fact for user 2.
	// Use the same embedding for both so they are equidistant.
	embedding := make([]float32, embeddingDimensions)
	for i := range embedding {
		embedding[i] = 0.1
	}
	serialized := serializeFloat32(embedding)

	_, err = database.Exec(`INSERT INTO users (discord_id, username, display_name) VALUES ('u1','a','a'),('u2','b','b')`)
	if err != nil {
		t.Fatal(err)
	}
	var userID1, userID2 int64
	database.QueryRow(`SELECT id FROM users WHERE discord_id='u1'`).Scan(&userID1)
	database.QueryRow(`SELECT id FROM users WHERE discord_id='u2'`).Scan(&userID2)

	_, err = database.Exec(`INSERT INTO facts (user_id, message_id, fact_text, is_active) VALUES (?,1,'fact for user1',1),(?,2,'fact for user2',1)`, userID1, userID2)
	if err != nil {
		t.Fatal(err)
	}
	var factID1, factID2 int64
	database.QueryRow(`SELECT id FROM facts WHERE user_id=?`, userID1).Scan(&factID1)
	database.QueryRow(`SELECT id FROM facts WHERE user_id=?`, userID2).Scan(&factID2)

	_, err = database.Exec(`INSERT INTO vec_facts (fact_id, embedding) VALUES (?,?),(?,?)`, factID1, serialized, factID2, serialized)
	if err != nil {
		t.Fatal(err)
	}

	// Query for user1 only — must not return user2's fact.
	results, err := findSimilarFacts(userID1, embedding)
	if err != nil {
		t.Fatal(err)
	}
	for _, r := range results {
		if r.FactText == "fact for user2" {
			t.Error("findSimilarFacts returned a fact belonging to another user")
		}
	}
}
```

**Step 2: Run to see the test fail (user2's fact leaks into results)**

```
/usr/local/go/bin/go test voltgpt/internal/memory -run "TestFindSimilarFacts_UserScoped" -v -timeout 30s
```

Expected: fails (user2's fact is returned).

**Step 3: Fix findSimilarFacts — switch to full-table scan**

In `internal/memory/consolidate.go`, replace the SQL in `findSimilarFacts` (lines 115–124):

```go
// Before:
rows, err := database.Query(`
    SELECT f.id, f.fact_text, vf.distance
    FROM vec_facts vf
    JOIN facts f ON f.id = vf.fact_id
    WHERE vf.embedding MATCH ?
      AND k = ?
      AND f.user_id = ?
      AND f.is_active = 1
    ORDER BY vf.distance
`, serializeFloat32(embedding), similarityLimit, userID)
```

```go
// After:
rows, err := database.Query(`
    SELECT f.id, f.fact_text, vec_distance_cosine(vf.embedding, ?) AS distance
    FROM vec_facts vf
    JOIN facts f ON f.id = vf.fact_id
    WHERE f.user_id = ?
      AND f.is_active = 1
    ORDER BY distance
    LIMIT ?
`, serializeFloat32(embedding), userID, similarityLimit*10)
```

Note: `LIMIT` is set to `similarityLimit * 10` (= 30) to retrieve more candidates before the Go-side distance filter (line 136: `if sf.Distance < distanceThreshold`). This compensates for the fact that a full-table scan returns all facts, not just nearby ones.

**Step 4: Fix Retrieve — switch to full-table scan**

In `internal/memory/retrieve.go`, replace the SQL in `Retrieve` (lines 45–55):

```go
// Before:
rows, err := database.Query(`
    SELECT f.fact_text, f.created_at, vf.distance
    FROM vec_facts vf
    JOIN facts f ON f.id = vf.fact_id
    JOIN users u ON u.id = f.user_id
    WHERE vf.embedding MATCH ?
      AND k = ?
      AND u.discord_id = ?
      AND f.is_active = 1
    ORDER BY vf.distance
`, serializeFloat32(embedding), retrievalLimit, discordID)
```

```go
// After:
rows, err := database.Query(`
    SELECT f.fact_text, f.created_at, vec_distance_cosine(vf.embedding, ?) AS distance
    FROM vec_facts vf
    JOIN facts f ON f.id = vf.fact_id
    JOIN users u ON u.id = f.user_id
    WHERE u.discord_id = ?
      AND f.is_active = 1
    ORDER BY distance
    LIMIT ?
`, serializeFloat32(embedding), discordID, retrievalLimit*10)
```

The Go-side filter (`if distance <= retrievalDistanceThreshold`) already bounds the returned set.

**Step 5: Run the test**

```
/usr/local/go/bin/go test voltgpt/internal/memory -run "TestFindSimilarFacts_UserScoped" -v -timeout 30s
```

Expected: passes (user2's fact not returned).

**Step 6: Run full memory test suite**

```
/usr/local/go/bin/go test voltgpt/internal/memory -timeout 60s -v
```

Expected: all pass.

**Step 7: Commit**

```bash
git add internal/memory/consolidate.go internal/memory/retrieve.go internal/memory/memory_test.go
git commit -m "fix(memory): use full-table scan for user-scoped sqlite-vec queries to prevent silent under-retrieval"
```

---

## Task 7: Fix playerTax returning negative value when player over-bets

**Context:** `internal/gamble/gamble.go` lines 237–242. When `betsPercentage(player, r)` exceeds 10, `betPercentage := 10 - g.betsPercentage(...)` goes negative, making `taxAmount` negative (a "bonus"). The caller already guards the display in `StatusEmbed`, but the function itself should not return negative values.

**Files:**
- Modify: `internal/gamble/gamble.go`
- Test: `internal/gamble/gamble_test.go`

**Step 1: Write the failing test**

Add to `internal/gamble/gamble_test.go`:

```go
func TestPlayerTax_OverBetIsZeroNotNegative(t *testing.T) {
	setupGame()
	player := makePlayer("p1", "Alice")
	GameState.Players = append(GameState.Players, player)
	GameState.AddRound()

	// Bet more than 10% of starting money (starting money = 1000 by default or whatever config.StartingMoney is).
	// Check what startingMoney is; for the test just add a round with an over-10% bet manually.
	r := GameState.CurrentRound()
	// Give the player a large bet: over 10% of their current money.
	r.Bets = append(r.Bets, Bet{By: player, On: player, Amount: 9999})
	GameState.Rounds[r.ID] = r

	tax := GameState.playerTax(player, GameState.CurrentRound())
	if tax < 0 {
		t.Errorf("playerTax returned negative value %d, want >= 0", tax)
	}
}
```

Note: `playerTax` and `betsPercentage` are unexported. Since the test file is `package gamble`, they are accessible.

**Step 2: Run to verify it fails**

```
/usr/local/go/bin/go test voltgpt/internal/gamble -run "TestPlayerTax_OverBetIsZeroNotNegative" -v -timeout 10s
```

Expected: fails with a negative tax value.

**Step 3: Apply the fix**

In `internal/gamble/gamble.go` line 239:

```go
// Before:
betPercentage := 10 - g.betsPercentage(player, r)
```

```go
// After:
betPercentage := max(0, 10-g.betsPercentage(player, r))
```

**Step 4: Run tests**

```
/usr/local/go/bin/go test voltgpt/internal/gamble -timeout 30s -v
```

Expected: all pass.

**Step 5: Commit**

```bash
git add internal/gamble/gamble.go internal/gamble/gamble_test.go
git commit -m "fix(gamble): clamp playerTax betPercentage to zero to prevent negative tax amount"
```

---

## Task 8: Fix Streamer.Stop() deadlock when goroutine exits early

**Context:** `internal/apis/gemini/chat.go` lines 29 and 76. `done` is an unbuffered channel (`make(chan bool)`). `Stop()` sends to it (`s.done <- true`). If the goroutine started by `Start()` has already exited (due to a panic or a context cancellation not handled in the select), the send blocks forever, hanging the request handler goroutine.

**Fix:** Make `done` a buffered channel of size 1 so `Stop()` never blocks regardless of the goroutine's state.

**Files:**
- Modify: `internal/apis/gemini/chat.go`
- Test: `internal/apis/gemini/chat_test.go`

**Step 1: Write a test that verifies Stop() does not block**

Add to `internal/apis/gemini/chat_test.go`:

```go
func TestStreamer_StopDoesNotDeadlock(t *testing.T) {
	// If done is unbuffered and the goroutine has not started, Stop() deadlocks.
	// With a buffered channel it must return immediately.
	s := NewStreamer(nil, nil)
	// Do NOT call Start() — simulate goroutine never running.
	done := make(chan struct{})
	go func() {
		defer close(done)
		// Drain the done channel so the goroutine side is satisfied.
		// With buffered channel, Stop sends without blocking even with no receiver.
		s.Stop()
	}()
	select {
	case <-done:
		// passed
	case <-time.After(2 * time.Second):
		t.Fatal("Stop() deadlocked — done channel is likely unbuffered")
	}
}
```

Import `"time"` if not already imported in the test file.

**Step 2: Run to verify it deadlocks (times out)**

```
/usr/local/go/bin/go test voltgpt/internal/apis/gemini -run "TestStreamer_StopDoesNotDeadlock" -v -timeout 5s
```

Expected: timeout / panic after 2 seconds (the goroutine calling `s.Stop()` blocks because `s.done <- true` has no receiver; `Flush` and `EmbedThoughtSignature` also call nil session methods but the send is what deadlocks).

Note: `s.Flush()` and `s.EmbedThoughtSignature()` will be called on a nil session after the channel send. Wrap the goroutine's `Stop()` in a recover or check: the important property is just that the send doesn't block. You can test the channel property directly:

```go
func TestStreamer_DoneChannelIsBuffered(t *testing.T) {
	s := NewStreamer(nil, nil)
	// A buffered channel send must not block when there is no receiver.
	select {
	case s.done <- true:
		// good: channel is buffered
	default:
		t.Fatal("done channel is unbuffered: send would block without a receiver")
	}
}
```

This simpler test verifies the channel property without calling Stop() (which would also call Flush on a nil session).

**Step 3: Apply the fix**

In `internal/apis/gemini/chat.go` line 40:

```go
// Before:
done: make(chan bool),
```

```go
// After:
done: make(chan bool, 1),
```

**Step 4: Run tests**

```
/usr/local/go/bin/go test voltgpt/internal/apis/gemini -run "TestStreamer_DoneChannelIsBuffered" -v -timeout 10s
```

Expected: passes.

**Step 5: Run full gemini test suite**

```
/usr/local/go/bin/go test voltgpt/internal/apis/gemini -timeout 30s -v
```

Expected: all tests pass (integration tests skip if `GEMINI_TOKEN` is unset).

**Step 6: Commit**

```bash
git add internal/apis/gemini/chat.go internal/apis/gemini/chat_test.go
git commit -m "fix(gemini): use buffered done channel in Streamer to prevent Stop() deadlock"
```

---

## Task 9: Fix consolidation loop returning early on REINFORCE, skipping remaining facts

**Context:** `internal/memory/consolidate.go` lines 81–86. When the first similar fact is REINFORCEd, the loop returns immediately. Any remaining similar facts (up to `similarityLimit=3`) are never checked, so a contradicting fact that should be INVALIDATEd is left active indefinitely.

**Fix:** On REINFORCE, record that reinforcement happened and continue the loop. Only avoid inserting the new fact at the end if at least one reinforcement occurred.

**Files:**
- Modify: `internal/memory/consolidate.go`
- Test: `internal/memory/memory_test.go`

**Step 1: Write the failing test**

Add to `internal/memory/memory_test.go`:

```go
func TestConsolidateAndStore_ReinforceDoesNotSkipInvalidate(t *testing.T) {
	// When the first similar fact is REINFORCED, the loop must continue
	// to check subsequent similar facts for INVALIDATE.
	// This test verifies that after a REINFORCE, a contradicting second fact
	// is still checked (we can verify by inspecting that the loop ran to completion).
	// Since decideAction calls Gemini, skip without token.
	if os.Getenv("MEMORY_GEMINI_TOKEN") == "" {
		t.Skip("MEMORY_GEMINI_TOKEN not set")
	}
	// Integration test: insert two existing facts for a user where the first
	// should be REINFORCED and the second INVALIDATED by the new fact.
	// Verify that after consolidation, the second fact is inactive.
	// (Detailed setup omitted — implement when Gemini token is available.)
}
```

For a pure unit test without Gemini, test the loop structure logic directly by reading the code. The structural fix is straightforward enough to verify by inspection. Add a comment-only test as a placeholder, then apply the fix.

**Step 2: Apply the fix**

In `internal/memory/consolidate.go`, replace lines 72–106:

```go
// Before:
for _, sf := range similar {
    action, err := decideAction(ctx, sf.FactText, factText)
    if err != nil {
        log.Printf("memory: consolidation decision failed for fact %d: %v", sf.ID, err)
        continue
    }

    switch action.Action {
    case "REINFORCE":
        if err := reinforceFact(sf.ID); err != nil {
            log.Printf("memory: failed to reinforce fact %d: %v", sf.ID, err)
        }
        return nil

    case "INVALIDATE":
        return replaceFact(sf.ID, userID, messageID, factText, embedding)

    case "MERGE":
        if action.MergedText == "" {
            log.Printf("memory: MERGE action returned empty merged_text, inserting as new fact")
            return insertFact(userID, messageID, factText, embedding)
        }
        mergedEmbedding, err := embed(ctx, action.MergedText)
        if err != nil {
            return fmt.Errorf("merge embedding failed: %w", err)
        }
        return replaceFact(sf.ID, userID, messageID, action.MergedText, mergedEmbedding)

    case "KEEP":
        continue
    }
}

return insertFact(userID, messageID, factText, embedding)
```

```go
// After:
reinforced := false
for _, sf := range similar {
    action, err := decideAction(ctx, sf.FactText, factText)
    if err != nil {
        log.Printf("memory: consolidation decision failed for fact %d: %v", sf.ID, err)
        continue
    }

    switch action.Action {
    case "REINFORCE":
        if err := reinforceFact(sf.ID); err != nil {
            log.Printf("memory: failed to reinforce fact %d: %v", sf.ID, err)
        }
        reinforced = true // continue loop — check remaining facts for contradictions

    case "INVALIDATE":
        return replaceFact(sf.ID, userID, messageID, factText, embedding)

    case "MERGE":
        if action.MergedText == "" {
            log.Printf("memory: MERGE action returned empty merged_text, inserting as new fact")
            return insertFact(userID, messageID, factText, embedding)
        }
        mergedEmbedding, err := embed(ctx, action.MergedText)
        if err != nil {
            return fmt.Errorf("merge embedding failed: %w", err)
        }
        return replaceFact(sf.ID, userID, messageID, action.MergedText, mergedEmbedding)

    case "KEEP":
        continue
    }
}

if reinforced {
    return nil
}
return insertFact(userID, messageID, factText, embedding)
```

**Step 3: Build and run memory tests**

```
/usr/local/go/bin/go build -o voltgpt && /usr/local/go/bin/go test voltgpt/internal/memory -timeout 60s -v
```

Expected: builds and all tests pass.

**Step 4: Commit**

```bash
git add internal/memory/consolidate.go internal/memory/memory_test.go
git commit -m "fix(memory): continue consolidation loop after REINFORCE to catch subsequent contradictions"
```

---

## Final verification

Run the full test suite with the race detector:

```
/usr/local/go/bin/go test -race ./... -timeout 120s
```

Expected: all tests pass, no race conditions reported.

Build the final binary:

```
/usr/local/go/bin/go build -o voltgpt
```

Expected: clean build.
